apiVersion: "sparkoperator.k8s.io/v1beta2"
kind: SparkApplication
metadata:
  name: bigdl-pyspark-ncf
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "10.239.45.10/arda/intelanalytics/bigdl-k8s-spark-3.1.2:2.1.0-SNAPSHOT-20220509"
  imagePullPolicy: Always
  mainApplicationFile: local:///bigdl2.0/data/spark_pandas.py
  arguments:
   - -f
   - /bigdl2.0/data/nyc_taxi.csv
  deps:
    pyFiles:
     - local:///opt/bigdl-2.1.0-SNAPSHOT/python/bigdl-spark_3.1.2-2.1.0-SNAPSHOT-python-api.zip
     - local:///bigdl2.0/data/transfer_learning.py
  sparkVersion: "3.1.2"
  batchScheduler: "volcano"
  restartPolicy:
    type: Never
  sparkConf:
    "spark.driver.host": "10.244.7.74"
    "spark.driver.port": "10008"
    "spark.cores.max": "64"
    "spark.kubernetes.authenticate.driver.serviceAccountName": "spark"
    "spark.kubernetes.driver.volumes.persistentVolumeClaim.nfsvolumeclaim.options.claimName": "nfsvolumeclaim"
    "spark.kubernetes.driver.volumes.persistentVolumeClaim.nfsvolumeclaim.mount.path": "/bigdl2.0/data"
    "spark.kubernetes.executor.volumes.persistentVolumeClaim.nfsvolumeclaim.options.claimName": "nfsvolumeclaim"
    "spark.kubernetes.executor.volumes.persistentVolumeClaim.nfsvolumeclaim.mount.path": "/bigdl2.0/data"
    "spark.archives": "file:///bigdl2.0/data/le/pandas-environment.tar.gz"
    "spark.pyspark.driver.python": "/bigdl2.0/data/le/python_env/bin/python"
    "spark.pyspark.python": "/bigdl2.0/data/le/python_env/bin/python"
    "spark.kubernetes.executor.deleteOnTermination": "false"
    "spark.kubernetes.file.upload.path": "/bigdl2.0/data/le/"
    "spark.shuffle.reduceLocality.enabled": "false"
    "spark.shuffle.blockTransferService": "nio"
    "spark.scheduler.minRegisteredResourcesRatio": "1.0"
    "spark.scheduler.maxRegisteredResourcesWaitingTime": "3600s"
    "spark.speculation": "false"
    "spark.serializer": "org.apache.spark.serializer.JavaSerializer"
    "spark.sql.catalogImplementation": "in-memory"
    "spark.driver.extraClassPath": "local:///opt/bigdl-2.1.0-SNAPSHOT/jars/*"
    "spark.executor.extraClassPath": "local:///opt/bigdl-2.1.0-SNAPSHOT/jars/*"    
  volumes:
  - name: nfs-storage
    persistentVolumeClaim:
      claimName: nfsvolumeclaim  
  driver:
    cores: 4
    memory: "50g"  
    labels:
      version: 3.1.2
    serviceAccount: spark
    securityContext:
      privileged: true
    javaOptions: "-Dderby.stream.error.file=/tmp"
    volumeMounts:
      - name: nfs-storage
        mountPath: /bigdl2.0/data
      - name: nfs-storage
        mountPath: /root/.kube/config
        subPath: kubeconfig
    env:
      - name: http_proxy
        value: "http://child-prc.intel.com:913"
      - name: https_proxy
        value: "http://child-prc.intel.com:913"
      - name: no_proxy
        value: "172.16.0.117"
  executor:
    cores: 16
    memory: "50g"
    instances: 1
    memory: "512m"    
    labels:
      version: 3.1.2
    serviceAccount: spark
    securityContext:
      privileged: true
    volumeMounts:
      - name: nfs-storage
        mountPath: /bigdl2.0/data
      - name: nfs-storage
        mountPath: /root/.kube/config
        subPath: kubeconfig
    env:
      - name: http_proxy
        value: "http://child-prc.intel.com:913"
      - name: https_proxy
        value: "http://child-prc.intel.com:913"
      - name: PYTHONHOME
        value: "/bigdl2.0/data/le/python_env"
      - name: BIGDL_HOME
        value: "/opt/bigdl-2.1.0-SNAPSHOT"
      - name: SPARK_HOME
        value: "/opt/spark"
      - name: no_proxy
        value: "172.16.0.117"
